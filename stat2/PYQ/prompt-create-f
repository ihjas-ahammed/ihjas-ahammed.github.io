{
    "topics": [
        "topic 1",
        "topic 2"
    ],
    "sections": [
        {
            "name": "Section A",
            "marks": 3,
            "questions": [
                "q1?",
                "q2"
            ],
            "topicNo": [
                1,
                2
            ]
        },
        {
            "name": "Section B",
            "marks": 5,
            "questions": [
                "q1",
                "q2"
            ],
            "topicNo": [
                1,
                2
            ]
        },
        {
            "name": "Section C",
            "marks": 10,
            "questions": [
                "q1",
                "q2"
            ],
            "topicNo": [
                1,
                2
            ]
        }
    ]
}

the topics and topicNo are a mapping from each question to the topic get topic names from the module


Section A must have 32 unique qs, secion B 20, section C, 
4

the topics and topicNo are a mapping from each question to the topic get topic names from the module

take approprate question fir given mark from below also create new questions from the notes given below the model questions based on the model questions if needed and recreate the json


# 1. Random Variables and Probability Distributions

1. What is the expected profit for a seller per item, if he is selling a large number of this item per day with various profits Rs. 2,
3 or 4 according to the buyer with a probability 0.2,
0.35 and 0.45 respectively?


# 2. Probability Mass Function (PMF) and Cumulative Distribution Function (CDF)

1. If the number shown when an unbiased die is thrown. Name the probability distribution of X. Write the p.m.f. of X.  


# 3. Mathematical Expectation

1.  For any random variable $X$, show that $V(X)$ is always non-negative. (Hint: Use the definition of variance in terms of expectation)


# 4. Moments

1. Explain two methods of finding raw moments of $X$, when the moment generating function of $X$ exists.
2. Show that the 1st central moment of a random variable $X$ is zero.


# 6. Binomial Distribution

1. Identify the parameters of $X$ following a binomial distribution with mean 12 and variance 3. (Use the formulas for mean and variance of a binomial distribution)
2. State and prove the additive property of binomial distribution. (This requires using the definition of binomial distribution and potentially the MGF)
3. If $P(X=2) = P(X=3)$, where $X$ follows a Poisson distribution, find the m.g.f. of $X$. (Use the PMF of the Poisson distribution and the definition of MGF)
4. What are the physical conditions for which the binomial distribution is used?


# 7. Poisson Distribution

1. If the variance of $X$ following a Poisson distribution is 5, find $P(X = 5)$. (Use the fact that mean and variance are equal in a Poisson distribution)  
3. Show that in a Poisson distribution with unit mean, the mean deviation about the mean is $\frac{
    2
}{e
}$ times the standard deviation. (This involves calculating the mean deviation and relating it to the standard deviation, using the PMF of the Poisson distribution.)
4.  A car hire firm has two cars which it hires out day by day. The number of demands for a car on each day is distributed as a Poisson variate with mean 1.5. Calculate the proportion of days on which (i) neither car is used and (ii) some demand is refused. (Apply the Poisson distribution to a real-world scenario).
5. Derive the Poisson distribution as a limiting case of the binomial distribution. (This requires a detailed mathematical derivation using limits.)

  
# 8. General Probability Concepts

1. Define (a) Random variable ; (b) Event.
4. Name the discrete distribution having the memoryless property. (Recall the unique property of one of the discussed distributions)
5. If $X \sim B(n, p)$, the distribution of $y = n - X$ is __________. (Think about the properties of the binomial distribution and transformations)
6. If $X$ and $Y$ are two independent variables, the conditional distribution of $X$ given $Y = y, f(x|y) =$ __________. (Consider the meaning of independence in terms of conditional probabilities)
8. Define mathematical expectation of a random variable.
9. What are the properties of the moment generating function?
11. Define binomial distribution.
12. State and prove the addition theorem of expectation.
14. Find the m.g.f. of the random variables whose moments are (i) $\mu_r' = (r+1)!2^r$ and (ii) $\mu_r = r!$  (Use the relationship between moments and the MGF).



# Discrete Random Variables and Their Probability Distribution

## 1. Random Variable and Probability Distribution

A random variable is a function from a sample space to a real number.

For example, consider tossing two coins at the same time. The sample space is:

$$ S = \{HH, HT, TH, TT\
} $$

Letâ€™s define a random variable $ x $ as the number of heads. The values of $ x $ and their probabilities are:

|       | HH  | HT  | TH  | TT  |
|-------|-----|-----|-----|-----|
| $ x $ | 2   | 1   | 1   | 0   |
| $ p $ | $ P(x=2) = \frac{
    1
}{
    4
} $ | $ P(x=1) = \frac{
    1
}{
    2
} $ | $ P(x=1) = \frac{
    1
}{
    2
} $ | $ P(x=0) = \frac{
    1
}{
    4
} $ |

This table represents the **probability distribution** (similar to a frequency distribution).

The **expectation** of $ x $, denoted $ E(x) $, is the mean of the probability distribution:

$$ E(x) = \sum x_i P(x = x_i) $$

In this case:

$$ E(x) = 2 \cdot \frac{
    1
}{
    4
} + 1 \cdot \frac{
    1
}{
    2
} + 1 \cdot \frac{
    1
}{
    2
} + 0 \cdot \frac{
    1
}{
    4
} = 0.5 + 0.5 + 0.5 + 0 = 1 $$

Random variables are also called *chance variables* or *stochastic variables*. If $ x_1 $ and $ x_2 $ are random variables and $ c $ is a constant, then:

1. $ c x_1 $ is a random variable
2. $ x_1 + x_2 $ is a random variable
3. $ x_1 - x_2 $ is a random variable
4. $ \max(x_1, x_2) $ is a random variable
5. $ \min(x_1, x_2) $ is a random variable

---

## 2. Probability Mass Functions

The probability distribution of a discrete random variable $ X $ is a list of distinct values $ x_i $ with their associated probabilities. The function $ f(x_i) $ or $ P(X = x_i) $ is called the **probability mass function (PMF)**, provided:

1. $ f(x_i) \geq 0 $
2. $ \sum f(x_i) = 1 $

A probability mass function can be represented as a histogram.

For the coin toss example, the PMF is:

- $ P(x = 0) = \frac{
    1
}{
    4
} $
- $ P(x = 1) = \frac{
    1
}{
    2
} $ (since $ P(HT) + P(TH) = \frac{
    1
}{
    4
} + \frac{
    1
}{
    4
} = \frac{
    1
}{
    2
} $ )
- $ P(x = 2) = \frac{
    1
}{
    4
} $

---

## 3. Distribution Functions

For any random variable $ X $, the **cumulative distribution function (CDF)** is defined as:

$$ F(x) = P(X \leq x) $$

If $ X $ is a discrete random variable with PMF $ P(x) $, then:

$$ F(x) = P(X \leq x) = \sum_{x_i \leq x
} P(x_i) $$

### Properties

If $ F(x) $ is the CDF of $ X $:

1. It is defined for all real values of $ x $.
2. $ 0 \leq F(x) \leq 1 $.
3. $ F(-\infty) = 0 $ and $ F(\infty) = 1 $.
4. If $ a < b $, then $ F(a) \leq F(b) $ (i.e., $ F $ is non-decreasing).
5. For discrete random variables, the graph of $ F(x) $ is a step function.

For the coin toss example:

- $ F(0) = P(x \leq 0) = P(x = 0) = \frac{
    1
}{
    4
} $
- $ F(1) = P(x \leq 1) = P(x = 0) + P(x = 1) = \frac{
    1
}{
    4
} + \frac{
    1
}{
    2
} = \frac{
    3
}{
    4
} $
- $ F(2) = P(x \leq 2) = P(x = 0) + P(x = 1) + P(x = 2) = \frac{
    1
}{
    4
} + \frac{
    1
}{
    2
} + \frac{
    1
}{
    4
} = 1 $

---

## 4. Mathematical Expectation

Every distribution of a random variable has associated parameters. The distribution function $ F(x) $ or the mass function $ f(x) $ completely characterizes the behavior of $ X $.

### Expectation of a Function of a Random Variable

Let $ g(x) $ be a function of a discrete random variable $ X $. The expected value is:

$$ E(g(x)) = \sum g(x_i) P(x_i) $$

### Properties of Expectation

1. **If $ C $ is a constant, $ E(C) = C $:**

   $$ E(C) = \sum C P(x_i) = C \sum P(x_i) = C \cdot 1 = C $$

2. **If $ g(x) $ is a function of $ X $ and $ C $ is a constant, then $ E[C g(x)
] = C E[g(x)
] $:**

   $$ E(C g(x)) = \sum C g(x_i) P(x_i) = C \sum g(x_i) P(x_i) = C E(g(x)) $$

3. **If $ a $ and $ b $ are constants, then $ E[aX + b
] = a E(X) + b $:**

   $$ E(aX + b) = \sum (a x_i + b) P(x_i) = a \sum x_i P(x_i) + b \sum P(x_i) = a E(X) + b $$

4. **If $ g(x) $ and $ h(x) $ are functions of $ X $, then $ E[g(x) + h(x)
] = E[g(x)
] + E[h(x)
] $:**

   $$ E(g(x) + h(x)) = \sum (g(x_i) + h(x_i)) P(x_i) = \sum g(x_i) P(x_i) + \sum h(x_i) P(x_i) = E(g(x)) + E(h(x)) $$

---

## 5. Moments

### Raw Moments

The $ r $-th raw moment about a value $ x_0 $ is:

$$ \mu_r' = E[(X - x_0)^r
] = \sum (x_i - x_0)^r P(x_i) $$

If $ x_0 = 0 $, the $ r $-th raw moment about the origin is:

$$ \mu_r' = E(X^r) = \sum x_i^r P(x_i) $$

For $ r = 1 $:

$$ \mu_1' = E(X) = \sum x_i P(x_i) $$

This is the mean of the random variable $ X $.

### Central Moments

The $ r $-th central moment (about the expected value) is:

$$ \mu_r = E[(X - E(X))^r
] $$

- For $ r = 1 $:

  $$ \mu_1 = E(X - E(X)) = E(X) - E(X) = 0 $$

- For $ r = 2 $:

  $$ \mu_2 = E[(X - E(X))^2
] $$

  This is the **variance** of $ X $.

### Relation Between Raw and Central Moments

The $ r $-th central moment can be expressed in terms of raw moments:

$$ \mu_r = \mu_r' - \binom{r
}{
    1
} \mu_{r-1
}' \mu_1' + \binom{r
}{
    2
} \mu_{r-2
}' (\mu_1')^2 + \cdots + (-1)^r (\mu_1')^r $$

- For $ r = 1 $:

  $$ \mu_1 = \mu_1' - \binom{
    1
}{
    1
} \mu_0' \mu_1' = \mu_1' - 1 \cdot 1 \cdot \mu_1' = 0 $$

- For $ r = 2 $:

  $$ \mu_2 = \mu_2' - \binom{
    2
}{
    1
} \mu_1' \mu_1' + \binom{
    2
}{
    2
} \mu_0' (\mu_1')^2 = \mu_2' - 2 (\mu_1')^2 + 1 \cdot (\mu_1')^2 = \mu_2' - (\mu_1')^2 $$

  This is the variance.

- For $ r = 3 $:

  $$ \mu_3 = \mu_3' - \binom{
    3
}{
    1
} \mu_2' \mu_1' + \binom{
    3
}{
    2
} \mu_1' (\mu_1')^2 - \binom{
    3
}{
    3
} (\mu_1')^3 = \mu_3' - 3 \mu_2' \mu_1' + 3 (\mu_1')^3 - (\mu_1')^3 = \mu_3' - 3 \mu_2' \mu_1' + 2 (\mu_1')^3 $$

---

## 6. Moment Generating Function (MGF)

The moment generating function of a random variable $ X $ is:

$$ M_X(t) = E(e^{tX
}) = \sum e^{t x_i
} P(x_i) $$

### Properties

1. If $ c $ is a constant: $ M_{cX
}(t) = M_X(ct) $
2. If $ a $ and $ b $ are constants: $ M_{aX+b
}(t) = e^{bt
} M_X(at) $
3. For independent $ X $ and $ Y $: $ M_{X+Y
}(t) = M_X(t) \cdot M_Y(t) $
4. $ M_X(0) = 1 $
5. The MGF uniquely determines the probability distribution (if it exists).
6. The $ r $-th raw moment is: $ \mu_r' = \frac{d^r
}{dt^r
} M_X(t) \big|_{t=0
} $

   - $ \frac{d
}{dt
} M_X(t) \big|_{t=0
} = \mu_1' $
   - $ \frac{d^2
}{dt^2
} M_X(t) \big|_{t=0
} = \mu_2' $

### Variance Relations

- $ \mu_2 = \mu_2' - (\mu_1')^2 $
- $ \mu_2 = E[(X - E(X))^2
] $
- $ V(aX) = a^2 V(X) $

---

## 7. Properties of Variance

1. $ V(aX) = E[(aX - E(aX))^2
] = a^2 V(X) $
2. For random variables $ X $ and $ Y $:

   $$ V(X + Y) = V(X) + V(Y) + 2 \text{Cov
}(X, Y) $$

3. If $ X $ and $ Y $ are independent:

   $$ V(X + Y) = V(X) + V(Y) $$

---

## 8. Covariance

The covariance of $ X $ and $ Y $ is:

$$ \text{Cov
}(X, Y) = E[(X - E(X))(Y - E(Y))
] = E(XY) - E(X)E(Y) $$

### Proof for Independent Variables

If $ X $ and $ Y $ are independent:

$$ E(XY) = E(X)E(Y) $$

Thus:

$$ \text{Cov
}(X, Y) = E(XY) - E(X)E(Y) = E(X)E(Y) - E(X)E(Y) = 0 $$

---

## 9. Binomial Distribution

A discrete random variable $ X $ follows a binomial distribution if its PMF is:

$$ P(X = x) = \binom{n
}{x
} p^x q^{n-x
}, \quad x = 0,
1,
2, \ldots, n $$

where $ q = 1 - p $, and otherwise $ P(X = x) = 0 $. Here, $ n $ and $ p $ are the parameters of the binomial distribution. We denote this as:

$$ X \sim B(n, p) $$

## Mean

$$ E(X) = \sum_{x=0
}^n x P(x) = \sum_{x=0
}^n x \binom{n
}{x
} p^x q^{n-x
} $$

Simplify:

$$ E(X) = \sum_{x=1
}^n x \frac{n!
}{x! (n-x)!
} p^x q^{n-x
} = \sum_{x=1
}^n n \frac{(n-1)!
}{(x-1)! (n-x)!
} p^x q^{n-x
} $$

Let $ k = x - 1 $:

$$ E(X) = n p \sum_{k=0
}^{n-1
} \binom{n-1
}{k
} p^k q^{n-1-k
} = n p (p + q)^{n-1
} = n p \cdot 1 = n p $$

**Mean = $ n p $**

## 10. Variance

$$ V(X) = E(X^2) - [E(X)
]^2 $$

First, compute $ E(X^2) $:

$$ E(X^2) = \sum_{x=0
}^n x^2 P(x) = \sum_{x=0
}^n [x(x-1) + x
] P(x) = \sum_{x=0
}^n x(x-1) P(x) + \sum_{x=0
}^n x P(x) $$

$$ = \sum_{x=2
}^n x(x-1) \binom{n
}{x
} p^x q^{n-x
} + n p $$

$$ = n(n-1) p^2 \sum_{x=2
}^n \binom{n-2
}{x-2
} p^{x-2
} q^{n-x
} + n p = n(n-1) p^2 (p + q)^{n-2
} + n p = n(n-1) p^2 + n p $$

Then:

$$ V(X) = E(X^2) - [E(X)
]^2 = [n(n-1) p^2 + n p
] - (n p)^2 = n^2 p^2 - n p^2 + n p - n^2 p^2 = n p (1 - p) = n p q $$

**Variance = $ n p q $**

## 11. Moment Generating Function

$$ M_X(t) = E(e^{tX
}) = \sum_{x=0
}^n e^{tx
} \binom{n
}{x
} p^x q^{n-x
} = \sum_{x=0
}^n \binom{n
}{x
} (p e^t)^x q^{n-x
} = (q + p e^t)^n $$

---

## 12. Poisson Distribution

A random variable $ X $ follows a Poisson distribution if its PMF is:

$$ P(X = x) = \frac{e^{-\lambda
    } \lambda^x
}{x!
}, \quad x = 0,
1,
2, \ldots $$

otherwise, $ P(X = x) = 0 $. Here, $ \lambda $ is the parameter.

### Mean and Variance

$$ E(X) = \sum_{x=0
}^\infty x \frac{e^{-\lambda
    } \lambda^x
}{x!
} = e^{-\lambda
} \sum_{x=1
}^\infty \frac{\lambda^x
}{(x-1)!
} = e^{-\lambda
} \lambda \sum_{k=0
}^\infty \frac{\lambda^k
}{k!
} = e^{-\lambda
} \lambda e^\lambda = \lambda $$

**Mean = $ \lambda $**

$$ E(X^2) = \sum_{x=0
}^\infty x^2 \frac{e^{-\lambda
    } \lambda^x
}{x!
} = \sum_{x=0
}^\infty [x(x-1) + x
] \frac{e^{-\lambda
    } \lambda^x
}{x!
} = e^{-\lambda
} \lambda^2 \sum_{x=2
}^\infty \frac{\lambda^{x-2
    }
}{(x-2)!
} + \lambda = \lambda^2 + \lambda $$

$$ V(X) = E(X^2) - [E(X)
]^2 = (\lambda^2 + \lambda) - \lambda^2 = \lambda $$

**Variance = $ \lambda $**

###  Moment Generating Function

$$ M_X(t) = E(e^{tX
}) = \sum_{x=0
}^\infty e^{tx
} \frac{e^{-\lambda
    } \lambda^x
}{x!
} = e^{-\lambda
} \sum_{x=0
}^\infty \frac{(\lambda e^t)^x
}{x!
} = e^{-\lambda
} e^{\lambda e^t
} = e^{\lambda (e^t - 1)
} $$

###  Moments: Poisson Distribution

#### Moment-Generating Function
The moment-generating function (MGF) for a random variable \( X \) is given as:
$$ M_X(t) = e^{-\lambda
} e^{\lambda e^t
} $$
This is the MGF of a Poisson random variable \( X \) with parameter \( \lambda \).

#### First Derivative
The first derivative of the MGF is computed to find the mean:
$$ M'_X(t) = \frac{d
}{dt
} M_X(t) = e^{-\lambda
} e^{\lambda e^t
} \cdot \lambda e^t $$
Evaluating at \( t = 0 \):
$$ M'_X(0) = e^{-\lambda
} e^{\lambda
} \cdot \lambda = \lambda $$
Thus, the mean of the distribution is:
$$ E[X
] = \lambda $$

#### Second Derivative
The second derivative of the MGF is computed to find the second moment:
$$ M''_X(t) = \frac{d
}{dt
} \left( e^{-\lambda
} e^{\lambda e^t
} \cdot \lambda e^t \right) $$
Using the product rule:
- Let \( u = e^{\lambda e^t
} \), \( v = \lambda e^t \)
- Derivatives:
  $$ \frac{du
}{dt
} = e^{\lambda e^t
} \cdot \lambda e^t $$
  $$ \frac{dv
}{dt
} = \lambda e^t $$
- Applying the product rule:
  $$ \frac{d
}{dt
} (u \cdot v) = u' v + u v' = (e^{\lambda e^t
} \cdot \lambda e^t) \cdot (\lambda e^t) + e^{\lambda e^t
} \cdot (\lambda e^t) $$
  $$ = e^{\lambda e^t
} \cdot \lambda e^t \left( \lambda e^t + 1 \right) $$
Thus:
$$ M''_X(t) = e^{-\lambda
} \left[ (e^{\lambda e^t
    } \cdot \lambda e^t) \cdot \lambda e^t + e^{\lambda e^t
    } \cdot \lambda e^t \right
] $$
Evaluating at \( t = 0 \):
$$ M''_X(0) = e^{-\lambda
} \left[ e^{\lambda
    } \cdot \lambda^2 + e^{\lambda
    } \cdot \lambda \right
] = e^{-\lambda
} e^{\lambda
} (\lambda^2 + \lambda) = \lambda^2 + \lambda $$
So, the second moment is:
$$ E[X^2
] = \lambda^2 + \lambda $$

#### Variance Calculation
The variance is calculated using the formula:
$$ V(X) = E[X^2
] - (E[X
])^2 $$
Substituting the values:
$$ V(X) = (\lambda^2 + \lambda) - \lambda^2 = \lambda $$

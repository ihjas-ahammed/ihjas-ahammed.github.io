# Discrete Random Variables and Their Probability Distribution

## Random Variable and Probability Distribution

A random variable is a function from a sample space to a real number.

For example, consider tossing two coins at the same time. The sample space is:

$$ S = \{HH, HT, TH, TT\} $$

Letâ€™s define a random variable $ x $ as the number of heads. The values of $ x $ and their probabilities are:

|       | HH  | HT  | TH  | TT  |
|-------|-----|-----|-----|-----|
| $ x $ | 2   | 1   | 1   | 0   |
| $ p $ | $ P(x=2) = \frac{1}{4} $ | $ P(x=1) = \frac{1}{2} $ | $ P(x=1) = \frac{1}{2} $ | $ P(x=0) = \frac{1}{4} $ |

This table represents the **probability distribution** (similar to a frequency distribution).

The **expectation** of $ x $, denoted $ E(x) $, is the mean of the probability distribution:

$$ E(x) = \sum x_i P(x = x_i) $$

In this case:

$$ E(x) = 2 \cdot \frac{1}{4} + 1 \cdot \frac{1}{2} + 1 \cdot \frac{1}{2} + 0 \cdot \frac{1}{4} = 0.5 + 0.5 + 0.5 + 0 = 1 $$

Random variables are also called *chance variables* or *stochastic variables*. If $ x_1 $ and $ x_2 $ are random variables and $ c $ is a constant, then:

1. $ c x_1 $ is a random variable
2. $ x_1 + x_2 $ is a random variable
3. $ x_1 - x_2 $ is a random variable
4. $ \max(x_1, x_2) $ is a random variable
5. $ \min(x_1, x_2) $ is a random variable

---

## Probability Mass Functions

The probability distribution of a discrete random variable $ X $ is a list of distinct values $ x_i $ with their associated probabilities. The function $ f(x_i) $ or $ P(X = x_i) $ is called the **probability mass function (PMF)**, provided:

1. $ f(x_i) \\geq 0 $
2. $ \sum f(x_i) = 1 $

A probability mass function can be represented as a histogram.

For the coin toss example, the PMF is:

- $ P(x = 0) = \frac{1}{4} $
- $ P(x = 1) = \frac{1}{2} $ (since $ P(HT) + P(TH) = \frac{1}{4} + \frac{1}{4} = \frac{1}{2} $ )
- $ P(x = 2) = \frac{1}{4} $

---

## Distribution Functions

For any random variable $ X $, the **cumulative distribution function (CDF)** is defined as:

$$ F(x) = P(X \leq x) $$

If $ X $ is a discrete random variable with PMF $ P(x) $, then:

$$ F(x) = P(X \leq x) = \sum_{x_i \leq x} P(x_i) $$

### Properties

If $ F(x) $ is the CDF of $ X $:

1. It is defined for all real values of $ x $.
2. $ 0 \leq F(x) \leq 1 $.
3. $ F(-\infty) = 0 $ and $ F(\infty) = 1 $.
4. If $ a < b $, then $ F(a) \leq F(b) $ (i.e., $ F $ is non-decreasing).
5. For discrete random variables, the graph of $ F(x) $ is a step function.

For the coin toss example:

- $ F(0) = P(x \leq 0) = P(x = 0) = \frac{1}{4} $
- $ F(1) = P(x \leq 1) = P(x = 0) + P(x = 1) = \frac{1}{4} + \frac{1}{2} = \frac{3}{4} $
- $ F(2) = P(x \leq 2) = P(x = 0) + P(x = 1) + P(x = 2) = \frac{1}{4} + \frac{1}{2} + \frac{1}{4} = 1 $

---

## Mathematical Expectation

Every distribution of a random variable has associated parameters. The distribution function $ F(x) $ or the mass function $ f(x) $ completely characterizes the behavior of $ X $.

### Expectation of a Function of a Random Variable

Let $ g(x) $ be a function of a discrete random variable $ X $. The expected value is:

$$ E(g(x)) = \sum g(x_i) P(x_i) $$

### Properties of Expectation

1. **If $ C $ is a constant, $ E(C) = C $:**

   $$ E(C) = \sum C P(x_i) = C \sum P(x_i) = C \cdot 1 = C $$

2. **If $ g(x) $ is a function of $ X $ and $ C $ is a constant, then $ E[C g(x)] = C E[g(x)] $:**

   $$ E(C g(x)) = \sum C g(x_i) P(x_i) = C \sum g(x_i) P(x_i) = C E(g(x)) $$

3. **If $ a $ and $ b $ are constants, then $ E[aX + b] = a E(X) + b $:**

   $$ E(aX + b) = \sum (a x_i + b) P(x_i) = a \sum x_i P(x_i) + b \sum P(x_i) = a E(X) + b $$

4. **If $ g(x) $ and $ h(x) $ are functions of $ X $, then $ E[g(x) + h(x)] = E[g(x)] + E[h(x)] $:**

   $$ E(g(x) + h(x)) = \sum (g(x_i) + h(x_i)) P(x_i) = \sum g(x_i) P(x_i) + \sum h(x_i) P(x_i) = E(g(x)) + E(h(x)) $$

---

## Moments

### Raw Moments

The $ r $-th raw moment about a value $ x_0 $ is:

$$ \mu_r' = E[(X - x_0)^r] = \sum (x_i - x_0)^r P(x_i) $$

If $ x_0 = 0 $, the $ r $-th raw moment about the origin is:

$$ \mu_r' = E(X^r) = \sum x_i^r P(x_i) $$

For $ r = 1 $:

$$ \mu_1' = E(X) = \sum x_i P(x_i) $$

This is the mean of the random variable $ X $.

### Central Moments

The $ r $-th central moment (about the expected value) is:

$$ \mu_r = E[(X - E(X))^r] $$

- For $ r = 1 $:

  $$ \mu_1 = E(X - E(X)) = E(X) - E(X) = 0 $$

- For $ r = 2 $:

  $$ \mu_2 = E[(X - E(X))^2] $$

  This is the **variance** of $ X $.

### Relation Between Raw and Central Moments

The $ r $-th central moment can be expressed in terms of raw moments:

$$ \mu_r = \mu_r' - \binom{r}{1} \mu_{r-1}' \mu_1' + \binom{r}{2} \mu_{r-2}' (\mu_1')^2 + \cdots + (-1)^r (\mu_1')^r $$

- For $ r = 1 $:

  $$ \mu_1 = \mu_1' - \binom{1}{1} \mu_0' \mu_1' = \mu_1' - 1 \cdot 1 \cdot \mu_1' = 0 $$

- For $ r = 2 $:

  $$ \mu_2 = \mu_2' - \binom{2}{1} \mu_1' \mu_1' + \binom{2}{2} \mu_0' (\mu_1')^2 = \mu_2' - 2 (\mu_1')^2 + 1 \cdot (\mu_1')^2 = \mu_2' - (\mu_1')^2 $$

  This is the variance.

- For $ r = 3 $:

  $$ \mu_3 = \mu_3' - \binom{3}{1} \mu_2' \mu_1' + \binom{3}{2} \mu_1' (\mu_1')^2 - \binom{3}{3} (\mu_1')^3 = \mu_3' - 3 \mu_2' \mu_1' + 3 (\mu_1')^3 - (\mu_1')^3 = \mu_3' - 3 \mu_2' \mu_1' + 2 (\mu_1')^3 $$

---

## Moment Generating Function (MGF)

The moment generating function of a random variable $ X $ is:

$$ M_X(t) = E(e^{tX}) = \sum e^{t x_i} P(x_i) $$

### Properties

1. If $ c $ is a constant: $ M_{cX}(t) = M_X(ct) $
2. If $ a $ and $ b $ are constants: $ M_{aX+b}(t) = e^{bt} M_X(at) $
3. For independent $ X $ and $ Y $: $ M_{X+Y}(t) = M_X(t) \cdot M_Y(t) $
4. $ M_X(0) = 1 $
5. The MGF uniquely determines the probability distribution (if it exists).
6. The $ r $-th raw moment is: $ \mu_r' = \frac{d^r}{dt^r} M_X(t) \big|_{t=0} $

   - $ \frac{d}{dt} M_X(t) \big|_{t=0} = \mu_1' $
   - $ \frac{d^2}{dt^2} M_X(t) \big|_{t=0} = \mu_2' $

### Variance Relations

- $ \mu_2 = \mu_2' - (\mu_1')^2 $
- $ \mu_2 = E[(X - E(X))^2] $
- $ V(aX) = a^2 V(X) $

---

## Properties of Variance

1. $ V(aX) = E[(aX - E(aX))^2] = a^2 V(X) $
2. For random variables $ X $ and $ Y $:

   $$ V(X + Y) = V(X) + V(Y) + 2 \\text{Cov}(X, Y) $$

3. If $ X $ and $ Y $ are independent:

   $$ V(X + Y) = V(X) + V(Y) $$

---

## Covariance

The covariance of $ X $ and $ Y $ is:

$$ \\text{Cov}(X, Y) = E[(X - E(X))(Y - E(Y))] = E(XY) - E(X)E(Y) $$

### Proof for Independent Variables

If $ X $ and $ Y $ are independent:

$$ E(XY) = E(X)E(Y) $$

Thus:

$$ \\text{Cov}(X, Y) = E(XY) - E(X)E(Y) = E(X)E(Y) - E(X)E(Y) = 0 $$

---

# Binomial Distribution

A discrete random variable $ X $ follows a binomial distribution if its PMF is:

$$ P(X = x) = \binom{n}{x} p^x q^{n-x}, \quad x = 0, 1, 2, \ldots, n $$

where $ q = 1 - p $, and otherwise $ P(X = x) = 0 $. Here, $ n $ and $ p $ are the parameters of the binomial distribution. We denote this as:

$$ X \sim B(n, p) $$

## Mean

$$ E(X) = \sum_{x=0}^n x P(x) = \sum_{x=0}^n x \binom{n}{x} p^x q^{n-x} $$

Simplify:

$$ E(X) = \sum_{x=1}^n x \frac{n!}{x! (n-x)!} p^x q^{n-x} = \sum_{x=1}^n n \frac{(n-1)!}{(x-1)! (n-x)!} p^x q^{n-x} $$

Let $ k = x - 1 $:

$$ E(X) = n p \sum_{k=0}^{n-1} \binom{n-1}{k} p^k q^{n-1-k} = n p (p + q)^{n-1} = n p \cdot 1 = n p $$

**Mean = $ n p $**

## Variance

$$ V(X) = E(X^2) - [E(X)]^2 $$

First, compute $ E(X^2) $:

$$ E(X^2) = \sum_{x=0}^n x^2 P(x) = \sum_{x=0}^n [x(x-1) + x] P(x) = \sum_{x=0}^n x(x-1) P(x) + \sum_{x=0}^n x P(x) $$

$$ = \sum_{x=2}^n x(x-1) \binom{n}{x} p^x q^{n-x} + n p $$

$$ = n(n-1) p^2 \sum_{x=2}^n \binom{n-2}{x-2} p^{x-2} q^{n-x} + n p = n(n-1) p^2 (p + q)^{n-2} + n p = n(n-1) p^2 + n p $$

Then:

$$ V(X) = E(X^2) - [E(X)]^2 = [n(n-1) p^2 + n p] - (n p)^2 = n^2 p^2 - n p^2 + n p - n^2 p^2 = n p (1 - p) = n p q $$

**Variance = $ n p q $**

## Moment Generating Function

$$ M_X(t) = E(e^{tX}) = \sum_{x=0}^n e^{tx} \binom{n}{x} p^x q^{n-x} = \sum_{x=0}^n \binom{n}{x} (p e^t)^x q^{n-x} = (q + p e^t)^n $$

---

# Poisson Distribution

A random variable $ X $ follows a Poisson distribution if its PMF is:

$$ P(X = x) = \frac{e^{-\lambda} \lambda^x}{x!}, \quad x = 0, 1, 2, \ldots $$

otherwise, $ P(X = x) = 0 $. Here, $ \lambda $ is the parameter.

## Mean and Variance

$$ E(X) = \sum_{x=0}^\infty x \frac{e^{-\lambda} \lambda^x}{x!} = e^{-\lambda} \sum_{x=1}^\infty \frac{\lambda^x}{(x-1)!} = e^{-\lambda} \lambda \sum_{k=0}^\infty \frac{\lambda^k}{k!} = e^{-\lambda} \lambda e^\lambda = \lambda $$

**Mean = $ \lambda $**

$$ E(X^2) = \sum_{x=0}^\infty x^2 \frac{e^{-\lambda} \lambda^x}{x!} = \sum_{x=0}^\infty [x(x-1) + x] \frac{e^{-\lambda} \lambda^x}{x!} = e^{-\lambda} \lambda^2 \sum_{x=2}^\infty \frac{\lambda^{x-2}}{(x-2)!} + \lambda = \lambda^2 + \lambda $$

$$ V(X) = E(X^2) - [E(X)]^2 = (\lambda^2 + \lambda) - \lambda^2 = \lambda $$

**Variance = $ \lambda $**

## Moment Generating Function

$$ M_X(t) = E(e^{tX}) = \sum_{x=0}^\infty e^{tx} \frac{e^{-\lambda} \lambda^x}{x!} = e^{-\lambda} \sum_{x=0}^\infty \frac{(\lambda e^t)^x}{x!} = e^{-\lambda} e^{\lambda e^t} = e^{\lambda (e^t - 1)} $$

## Moments: Poisson Distribution

### Moment-Generating Function
The moment-generating function (MGF) for a random variable \( X \) is given as:
$$ M_X(t) = e^{-\lambda} e^{\lambda e^t} $$
This is the MGF of a Poisson random variable \( X \) with parameter \( \lambda \).

### First Derivative
The first derivative of the MGF is computed to find the mean:
$$ M'_X(t) = \frac{d}{dt} M_X(t) = e^{-\lambda} e^{\lambda e^t} \cdot \lambda e^t $$
Evaluating at \( t = 0 \):
$$ M'_X(0) = e^{-\lambda} e^{\lambda} \cdot \lambda = \lambda $$
Thus, the mean of the distribution is:
$$ E[X] = \lambda $$

### Second Derivative
The second derivative of the MGF is computed to find the second moment:
$$ M''_X(t) = \frac{d}{dt} \left( e^{-\lambda} e^{\lambda e^t} \cdot \lambda e^t \right) $$
Using the product rule:
- Let \( u = e^{\lambda e^t} \), \( v = \lambda e^t \)
- Derivatives:
  $$ \frac{du}{dt} = e^{\lambda e^t} \cdot \lambda e^t $$
  $$ \frac{dv}{dt} = \lambda e^t $$
- Applying the product rule:
  $$ \frac{d}{dt} (u \cdot v) = u' v + u v' = (e^{\lambda e^t} \cdot \lambda e^t) \cdot (\lambda e^t) + e^{\lambda e^t} \cdot (\lambda e^t) $$
  $$ = e^{\lambda e^t} \cdot \lambda e^t \left( \lambda e^t + 1 \right) $$
Thus:
$$ M''_X(t) = e^{-\lambda} \left[ (e^{\lambda e^t} \cdot \lambda e^t) \cdot \lambda e^t + e^{\lambda e^t} \cdot \lambda e^t \right] $$
Evaluating at \( t = 0 \):
$$ M''_X(0) = e^{-\lambda} \left[ e^{\lambda} \cdot \lambda^2 + e^{\lambda} \cdot \lambda \right] = e^{-\lambda} e^{\lambda} (\lambda^2 + \lambda) = \lambda^2 + \lambda $$
So, the second moment is:
$$ E[X^2] = \lambda^2 + \lambda $$

### Variance Calculation
The variance is calculated using the formula:
$$ V(X) = E[X^2] - (E[X])^2 $$
Substituting the values:
$$ V(X) = (\lambda^2 + \lambda) - \lambda^2 = \lambda $$

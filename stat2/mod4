
# Sampling Distribution

## Parameter and Statistic

*   Any measure calculated on the basis of **population** values is called a **parameter**.
    *   eg: Population mean $\mu$, Population variance $\sigma^2$, Population SD $\sigma$, Population correlation coefficient $\rho$.
*   Any measure computed on the basis of **sample** value is called **Statistic**.
    *   eg: Sample mean $\bar{x}$, Sample variance $s^2$, Sample SD $s$, Sample Correlation coefficient $r$.

## Sampling Distribution

Suppose we draw all possible samples each of size '$n$' from the same Population of size $N$. And if for each sample the value of the statistic is calculated, a series of values of the statistic will be obtained.
The probability distribution of this statistic is called **sampling distribution**.

ie Let $X_1, X_2, \dots, X_n$ represent a random sample of size $n$.
Let $t = g(x_1, x_2, \dots, x_n)$ being a function of these RV. The probability distribution of $t$ is the sampling distribution. If $t$ is a statistic, its distribution is the sampling distribution, denoted as $f(t)$.

## Standard Error (SE)

The standard deviation of the sampling distribution of a statistic is called **Standard Error** (SE).

$$
SE(t) = \sqrt{V(t)} = \sqrt{E(t^2) - [E(t)]^2}
$$

## Uses of SE

1.  Since SE is inversely proportional to the sample size $n$, it is very helpful in the determination of the proper size of the sample to be taken to estimate the parameter.
2.  It is used for testing a given hypothesis.
3.  SE gives an idea about the reliability of a sample. The reciprocal of SE is a measure of reliability of the sample.
4.  SE can be used to determine the confidence limits of population parameters.

## Sample Mean

Let $X_1, X_2, \dots, X_n$ be a random sample of size $n$ drawn from $N(\mu, \sigma^2)$.
Let $\bar{x}$ be the sample mean.

*(Note: The distribution of $\bar{x}$ is $N(\mu, \sigma^2/n)$)*

## Moment Generating Function of $N(\mu, \sigma^2/n)$

*(This is the MGF of the sample mean $\bar{x}$)*
$$
M_{\bar{x}}(t) = E[e^{t\bar{x}}] = e^{\mu t + \frac{1}{2} (\frac{\sigma^2}{n}) t^2}
$$
From MGF:
$$
E(\bar{x}) = \mu
$$
$$
V(\bar{x}) = \frac{\sigma^2}{n}
$$

---

# Chi-Square Distribution ($\chi^2$)

A continuous RV $X$ is said to follow a chi-square distribution with $n$ degrees of freedom (d(f)) if its PDF is given by:
$$
f(x) =
\begin{cases}
\frac{1}{2^{n/2} \Gamma(n/2)} e^{-x/2} x^{n/2 - 1} & , 0 < x < \infty \\
0 & , \\text{otherwise}
\end{cases}
$$
Here $n$ is the parameter of the distribution and we write $X \sim \chi^2(n)$.

## Mean and Variance of $\chi^2(n)$

**Mean:**
$$
E(X) = \int_0^\infty x f(x) dx = \frac{1}{2^{n/2} \Gamma(n/2)} \int_0^\infty x \cdot e^{-x/2} x^{n/2 - 1} dx
$$
$$
= \frac{1}{2^{n/2} \Gamma(n/2)} \int_0^\infty e^{-x/2} x^{(n/2+1) - 1} dx
$$
Using the Gamma integral $\int_0^\infty e^{-ax} x^{p-1} dx = \frac{\Gamma(p)}{a^p}$, with $a=1/2, p=n/2+1$:
$$
= \frac{1}{2^{n/2} \Gamma(n/2)} \frac{\Gamma(n/2+1)}{(1/2)^{n/2+1}} = \frac{1}{2^{n/2} \Gamma(n/2)} \frac{(n/2)\Gamma(n/2)}{1/2^{n/2+1}} = \frac{n/2}{1/2} = n
$$
$$
E(X) = n
$$

**Variance:**
$$
V(X) = E(X^2) - [E(X)]^2
$$
$$
E(X^2) = \int_0^\infty x^2 f(x) dx = \frac{1}{2^{n/2} \Gamma(n/2)} \int_0^\infty x^2 e^{-x/2} x^{n/2 - 1} dx
$$
$$
= \frac{1}{2^{n/2} \Gamma(n/2)} \int_0^\infty e^{-x/2} x^{(n/2+2) - 1} dx
$$
Using the Gamma integral with $a=1/2, p=n/2+2$:
$$
= \frac{1}{2^{n/2} \Gamma(n/2)} \frac{\Gamma(n/2+2)}{(1/2)^{n/2+2}} = \frac{1}{2^{n/2} \Gamma(n/2)} \frac{(n/2+1)(n/2)\Gamma(n/2)}{1/2^{n/2+2}}
$$
$$
= \frac{(n/2+1)(n/2)}{1/2^2} = \frac{(\frac{n+2}{2})(\frac{n}{2})}{1/4} = 4 \frac{n(n+2)}{4} = n(n+2)
$$
$$
V(X) = E(X^2) - [E(X)]^2 = n(n+2) - n^2 = n^2 + 2n - n^2 = 2n
$$
$$
V(X) = 2n
$$

## Moment Generating Function (MGF) of $\chi^2(n)$

$$
M_X(t) = E[e^{tx}] = \int_0^\infty e^{tx} f(x) dx
$$
$$
= \int_0^\infty e^{tx} \frac{1}{2^{n/2} \Gamma(n/2)} e^{-x/2} x^{n/2 - 1} dx
$$
$$
= \frac{1}{2^{n/2} \Gamma(n/2)} \int_0^\infty e^{-x(1/2 - t)} x^{n/2 - 1} dx
$$
Using the Gamma integral with $a=(1/2 - t) = \frac{1-2t}{2}$, $p=n/2$. This requires $a>0$, so $1-2t > 0 \implies t < 1/2$.
$$
= \frac{1}{2^{n/2} \Gamma(n/2)} \frac{\Gamma(n/2)}{(\frac{1-2t}{2})^{n/2}} = \frac{1}{2^{n/2}} \frac{1}{(\frac{1-2t}{2})^{n/2}}
$$
$$
= \frac{1}{2^{n/2}} \frac{2^{n/2}}{(1-2t)^{n/2}} = (1-2t)^{-n/2}
$$
$$
M_X(t) = (1-2t)^{-n/2}, \quad \\text{for } t < 1/2
$$

## Additive Property

If $X_1 \sim \chi^2(n_1)$ and $X_2 \sim \chi^2(n_2)$ are independent $\chi^2$ distributions, then $X_1 + X_2$ will have a $\chi^2$ distribution with $n_1 + n_2$ degrees of freedom.
i.e., $X_1 + X_2 \sim \chi^2(n_1 + n_2)$.

**Proof:**
Given $X_1 \sim \chi^2(n_1)$ and $X_2 \sim \chi^2(n_2)$ are independent.
The MGF of $X_1$ is $M_{X_1}(t) = (1-2t)^{-n_1/2}$.
The MGF of $X_2$ is $M_{X_2}(t) = (1-2t)^{-n_2/2}$.
Since they are independent, the MGF of the sum $X_1 + X_2$ is the product of their MGFs:
$$
M_{X_1+X_2}(t) = M_{X_1}(t) \cdot M_{X_2}(t) = (1-2t)^{-n_1/2} \cdot (1-2t)^{-n_2/2}
$$
$$
= (1-2t)^{-(n_1/2 + n_2/2)} = (1-2t)^{-(n_1+n_2)/2}
$$
Hence, $M_{X_1+X_2}(t)$ is the MGF of a $\chi^2$ variate with $(n_1+n_2)$ degrees of freedom.
Therefore, $X_1 + X_2 \sim \chi^2(n_1 + n_2)$.

## Note

*   $\chi^2$ statistic is the sum of squares of standard normal variates.
    ie, if $X_i \sim N(\mu, \sigma^2)$ for $i=1, \dots, n$, then $Z_i = \frac{X_i - \mu}{\sigma} \sim N(0,1)$.
    Then $\chi^2 = \sum_{i=1}^n (\frac{X_i - \mu}{\sigma})^2 = \sum_{i=1}^n Z_i^2 \sim \chi^2(n)$.
*   If $X \sim N(\mu, \sigma^2)$, then $(\frac{X-\mu}{\sigma})^2 \sim \chi^2(1)$.

---

# Student's t-Distribution

If $Z$ is a standard normal variate ($Z \sim N(0,1)$) and $Y$ is a $\chi^2$ variate with $n$ degrees of freedom ($Y \sim \chi^2(n)$), and if $Z$ and $Y$ are independent, then the statistic $t$ defined by
$$
t = \frac{Z}{\sqrt{Y/n}}
$$
follows Student's t-distribution with $n$ degrees of freedom.

The PDF of the t-distribution is given by:
$$
f(t) = \frac{\Gamma(\frac{n+1}{2})}{\sqrt{n\pi} \Gamma(\frac{n}{2})} \left(1 + \frac{t^2}{n}\right)^{-\frac{n+1}{2}}, \quad -\infty < t < \infty
$$
*(Note: The constant term can also be written using the Beta function: $\frac{1}{\sqrt{n} B(\frac{1}{2}, \frac{n}{2})}$. Recall $\Gamma(1/2)=\sqrt{\pi}$ and $B(m,n) = \frac{\Gamma(m)\Gamma(n)}{\Gamma(m+n)}$)*

## Characteristics of t-Distribution

Let $v$ denote the degrees of freedom ($v=n$).
1.  The curve is symmetrical about $t=0$, bell-shaped, and unimodal.
2.  The mode of the t-distribution is at $t=0$.
3.  The mean of the t-distribution is $0$ (for $v > 1$).
4.  Odd order central moments are zero (due to symmetry).
5.  Even order central moments ($\mu_{2r}$) are given by:
    $$
    \mu_{2r} = E(t^{2r}) = \frac{v^r (1 \cdot 3 \cdot 5 \dots (2r-1))}{(v-2)(v-4)\dots(v-2r)}, \quad \\text{for } v > 2r
    $$
6.  Variance ($\mu_2$) = $V(t)$: Setting $r=1$ (requires $v>2$)
    $$
    \\text{Variance} = \mu_2 = \frac{v}{v-2}, \quad \\text{for } v > 2
    $$
7.  The MGF of the t-distribution does not exist.

## Example of Statistic Following Student's t-distribution

Let $X \sim N(\mu, \sigma^2)$. Let $\bar{x}$ be the sample mean and $s^2$ be the sample variance of a random sample of size $n$.
Since $Z = \frac{\bar{x} - \mu}{\sigma/\sqrt{n}} \sim N(0,1)$ and $Y = \frac{(n-1)s^2}{\sigma^2} \sim \chi^2(n-1)$, and $Z$ and $Y$ are independent, then
$$
t = \frac{Z}{\sqrt{Y/(n-1)}} = \frac{\frac{\bar{x} - \mu}{\sigma/\sqrt{n}}}{\sqrt{\frac{(n-1)s^2/\sigma^2}{(n-1)}}} = \frac{\frac{\bar{x} - \mu}{\sigma/\sqrt{n}}}{\sqrt{s^2/\sigma^2}} = \frac{\frac{\bar{x} - \mu}{\sigma/\sqrt{n}}}{s/\sigma} = \frac{\bar{x} - \mu}{s/\sqrt{n}}
$$
This statistic $t = \frac{\bar{x} - \mu}{s/\sqrt{n}}$ follows a t-distribution with $(n-1)$ degrees of freedom.

---

# Snedecor's F-Distribution

If $U$ and $V$ are two independent chi-square variates with $n_1$ and $n_2$ degrees of freedom respectively, then the F-statistic is defined as the ratio of two independent chi-square variates divided by their respective degrees of freedom.
$$
F = \frac{U/n_1}{V/n_2}
$$
The statistic $F$ follows Snedecor's F-distribution with $(n_1, n_2)$ degrees of freedom, denoted as $F \sim F(n_1, n_2)$.

The PDF of the F-distribution is given by:
$$
f(F) = \frac{(n_1/n_2)^{n_1/2}}{B(n_1/2, n_2/2)} \frac{F^{n_1/2 - 1}}{(1 + \frac{n_1}{n_2} F)^{(n_1+n_2)/2}}, \quad F > 0
$$
where $B(m,n) = \frac{\Gamma(m)\Gamma(n)}{\Gamma(m+n)}$.
The F-distribution has 2 parameters, $n_1$ (numerator degrees of freedom) and $n_2$ (denominator degrees of freedom).

## Note: Application to Sample Variances

Let $s_1^2$ and $s_2^2$ be the sample variances from two independent random samples of sizes $n_1$ and $n_2$ taken from normal populations $N(\mu_1, \sigma_1^2)$ and $N(\mu_2, \sigma_2^2)$ respectively.
We know that $\frac{(n_1-1)s_1^2}{\sigma_1^2} \sim \chi^2(n_1-1)$ and $\frac{(n_2-1)s_2^2}{\sigma_2^2} \sim \chi^2(n_2-1)$.
Then the statistic
$$
F = \frac{\frac{(n_1-1)s_1^2}{\sigma_1^2} / (n_1-1)}{\frac{(n_2-1)s_2^2}{\sigma_2^2} / (n_2-1)} = \frac{s_1^2 / \sigma_1^2}{s_2^2 / \sigma_2^2}
$$
follows an F-distribution with $(n_1-1, n_2-1)$ degrees of freedom.
If we assume $\sigma_1^2 = \sigma_2^2$, then the statistic
$$
F = \frac{s_1^2}{s_2^2} \sim F(n_1-1, n_2-1)
$$

## Characteristics of F-Distribution

1.  The mean of the F-distribution is:
    $$
    E(F) = \frac{n_2}{n_2-2}, \quad \\text{for } n_2 > 2
    $$
    (No mean exists for $n_2 \le 2$)
2.  The variance of the F-distribution is:
    $$
    V(F) = \frac{2n_2^2(n_1+n_2-2)}{n_1(n_2-2)^2(n_2-4)}, \quad \\text{for } n_2 > 4
    $$
    (No variance exists for $n_2 \le 4$)
3.  It is non-symmetric and positively skewed.
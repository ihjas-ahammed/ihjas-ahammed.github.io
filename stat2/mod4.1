

# Sampling Distributions and Related Concepts

## 1. Parameter vs. Statistic

*   **Parameter:** A numerical measure calculated using all the values in a **population**. Parameters describe characteristics of the population.
    *   *Examples:* Population mean ($\mu$), Population variance ($\sigma^2$), Population standard deviation ($\sigma$), Population correlation coefficient ($\rho$).
*   **Statistic:** A numerical measure calculated using values from a **sample**. Statistics are used to estimate population parameters or describe the sample.
    *   *Examples:* Sample mean ($\bar{x}$), Sample variance ($s^2$), Sample standard deviation ($s$), Sample correlation coefficient ($r$).

*Key Idea:* Parameters come from populations, Statistics come from samples.

## 2. Sampling Distribution

Imagine we could take *all possible samples* of a fixed size '$n$' from a population of size $N$. For each sample, we calculate a specific statistic (like the sample mean $\bar{x}$). The collection of all these statistic values forms a distribution.

*   **Definition:** The **sampling distribution** of a statistic is the probability distribution of all possible values of that statistic, computed from samples of the same size $n$ drawn from the same population.

*   *Formal Definition:* Let $X_1, X_2, \dots, X_n$ be a random sample of size $n$. Let $t = g(X_1, X_2, \dots, X_n)$ be a statistic (a function of the sample). The probability distribution of the random variable $t$, denoted $f(t)$, is its sampling distribution.

## 3. Standard Error (SE)

*   **Definition:** The standard deviation of the sampling distribution of a statistic is called its **Standard Error (SE)**. It measures the variability or spread of the statistic's values across all possible samples.

*   **Formula:**
    $$
    SE(t) = \sqrt{\text{Var}(t)} = \sqrt{E[t^2] - (E[t])^2}
    $$
    where $t$ is the statistic.

## 4. Uses of Standard Error (SE)

1.  **Sample Size Determination:** SE is typically inversely proportional to the square root of the sample size ($\sqrt{n}$). A smaller desired SE requires a larger sample size, helping researchers determine how large a sample is needed for a certain precision.
2.  **Hypothesis Testing:** SE is crucial in constructing test statistics to determine if sample results are statistically significant.
3.  **Reliability/Precision:** A smaller SE indicates that the sample statistic is likely closer to the population parameter, implying greater reliability or precision. The reciprocal of SE ($1/SE$) can be seen as a measure of reliability.
4.  **Confidence Intervals:** SE is used to calculate confidence intervals, which provide a range of plausible values for the population parameter.

## 5. Sampling Distribution of the Sample Mean ($\bar{x}$)

Let $X_1, X_2, \dots, X_n$ be a random sample of size $n$ drawn from a Normally distributed population with mean $\mu$ and variance $\sigma^2$, i.e., $X_i \sim N(\mu, \sigma^2)$.

*   **Distribution of $\bar{x}$:** The sample mean $\bar{x} = \frac{1}{n}\sum_{i=1}^n X_i$ also follows a Normal distribution:
    $$
    \bar{x} \sim N\left(\mu, \frac{\sigma^2}{n}\right)
    $$
*   **Mean of $\bar{x}$:** The expected value of the sample mean is the population mean:
    $$
    E[\bar{x}] = \mu
    $$
    (This means $\bar{x}$ is an unbiased estimator of $\mu$).
*   **Variance of $\bar{x}$:** The variance of the sample mean is the population variance divided by the sample size:
    $$
    \text{Var}(\bar{x}) = \frac{\sigma^2}{n}
    $$
*   **Standard Error of $\bar{x}$:**
    $$
    SE(\bar{x}) = \sqrt{\text{Var}(\bar{x})} = \frac{\sigma}{\sqrt{n}}
    $$
*   **Moment Generating Function (MGF) of $\bar{x}$:** The MGF confirms the distribution, mean, and variance:
    $$
    M_{\bar{x}}(t) = E[e^{t\bar{x}}] = e^{\mu t + \frac{1}{2} \left(\frac{\sigma^2}{n}\right) t^2}
    $$
    (This is the MGF of a $N(\mu, \sigma^2/n)$ distribution).

*(Note: By the Central Limit Theorem, $\bar{x}$ is approximately $N(\mu, \sigma^2/n)$ even if the population is not Normal, provided $n$ is sufficiently large).*

---

# Chi-Square Distribution ($\chi^2$)

A continuous random variable $X$ follows a **Chi-Square distribution** with $n$ **degrees of freedom (df)**, denoted $X \sim \chi^2(n)$, if its Probability Density Function (PDF) is:

$$
f(x; n) =
\begin{cases}
\frac{1}{2^{n/2} \Gamma(n/2)} x^{n/2 - 1} e^{-x/2} & , \text{ for } x > 0 \\
0 & , \text{ otherwise}
\end{cases}
$$

Here, $n$ (a positive integer) is the parameter (degrees of freedom), and $\Gamma(\cdot)$ is the Gamma function.

## Mean and Variance of $\chi^2(n)$

*   **Mean:** $E[X] = n$
*   **Variance:** $\text{Var}(X) = 2n$

*(Derivations omitted here, but they involve integrating $x \cdot f(x)$ and $x^2 \cdot f(x)$ using properties of the Gamma function).*

## Moment Generating Function (MGF) of $\chi^2(n)$

The MGF of a $\chi^2(n)$ random variable $X$ is:
$$
M_X(t) = E[e^{tX}] = (1 - 2t)^{-n/2}, \quad \text{for } t < 1/2
$$

## Additive Property of Chi-Square

If $X_1 \sim \chi^2(n_1)$ and $X_2 \sim \chi^2(n_2)$ are **independent** Chi-Square random variables, then their sum also follows a Chi-Square distribution with degrees of freedom equal to the sum of their individual degrees of freedom:
$$
X_1 + X_2 \sim \chi^2(n_1 + n_2)
$$
*(Proof uses the property that the MGF of a sum of independent variables is the product of their individual MGFs).*

## Connection to Standard Normal Distribution

The Chi-Square distribution arises fundamentally from the Standard Normal distribution:

*   If $Z \sim N(0,1)$ (a standard normal variable), then $Z^2 \sim \chi^2(1)$.
*   If $Z_1, Z_2, \dots, Z_n$ are independent standard normal variables ($Z_i \sim N(0,1)$), then the sum of their squares follows a Chi-Square distribution with $n$ degrees of freedom:
    $$
    \sum_{i=1}^n Z_i^2 \sim \chi^2(n)
    $$
*   If $X_1, \dots, X_n$ are independent $N(\mu, \sigma^2)$ variables, then:
    $$
    \sum_{i=1}^n \left(\frac{X_i - \mu}{\sigma}\right)^2 \sim \chi^2(n)
    $$

---

# Student's t-Distribution

Suppose $Z$ is a standard normal random variable ($Z \sim N(0,1)$) and $Y$ is an independent Chi-Square random variable with $n$ degrees of freedom ($Y \sim \chi^2(n)$). The **t-statistic** is defined as:

$$
t = \frac{Z}{\sqrt{Y/n}}
$$

This statistic $t$ follows **Student's t-distribution** with $n$ degrees of freedom, denoted $t \sim t(n)$ or $t_n$.

## PDF of t-Distribution

The PDF of the t-distribution with $n$ degrees of freedom is:
$$
f(t; n) = \frac{\Gamma\left(\frac{n+1}{2}\right)}{\sqrt{n\pi} \Gamma\left(\frac{n}{2}\right)} \left(1 + \frac{t^2}{n}\right)^{-\frac{n+1}{2}}, \quad -\infty < t < \infty
$$
*(Note: The constant can be written using the Beta function as $\frac{1}{\sqrt{n} B(1/2, n/2)}$).*

## Characteristics of t-Distribution

Let $n$ be the degrees of freedom.
1.  **Symmetry:** The distribution is symmetric about $t=0$, bell-shaped, and unimodal (similar to the Normal distribution but with heavier tails).
2.  **Mode:** The mode is at $t=0$.
3.  **Mean:** $E[t] = 0$, provided $n > 1$.
4.  **Variance:** $\text{Var}(t) = \frac{n}{n-2}$, provided $n > 2$. (Note: Variance is > 1, larger than $N(0,1)$).
5.  **Moments:** Odd-order central moments are 0 (due to symmetry). Even-order central moments $\mu_{2r} = E[t^{2r}]$ exist only if $n > 2r$. For example, the fourth moment (kurtosis related) exists only if $n > 4$.
6.  **MGF:** The Moment Generating Function does not exist for the t-distribution.
7.  **Asymptotic Behavior:** As $n \to \infty$, the t-distribution approaches the standard normal distribution $N(0,1)$.

## Important Application: Inference about Population Mean (Ïƒ unknown)

Let $X_1, \dots, X_n$ be a random sample from $N(\mu, \sigma^2)$ where **$\sigma^2$ is unknown**. Let $\bar{x}$ be the sample mean and $s^2$ be the **sample variance** ($s^2 = \frac{1}{n-1}\sum(X_i - \bar{x})^2$).
We know:
*   $Z = \frac{\bar{x} - \mu}{\sigma/\sqrt{n}} \sim N(0,1)$
*   $Y = \frac{(n-1)s^2}{\sigma^2} \sim \chi^2(n-1)$
*   $\bar{x}$ and $s^2$ are independent when sampling from a Normal distribution.

Therefore, the statistic:
$$
t = \frac{\frac{\bar{x} - \mu}{\sigma/\sqrt{n}}}{\sqrt{\frac{(n-1)s^2/\sigma^2}{n-1}}} = \frac{\frac{\bar{x} - \mu}{\sigma/\sqrt{n}}}{\sqrt{s^2/\sigma^2}} = \frac{\frac{\bar{x} - \mu}{\sigma/\sqrt{n}}}{s/\sigma} = \frac{\bar{x} - \mu}{s/\sqrt{n}}
$$
follows a **t-distribution with $(n-1)$ degrees of freedom**:
$$
\frac{\bar{x} - \mu}{s/\sqrt{n}} \sim t(n-1)
$$
This is fundamental for constructing confidence intervals and hypothesis tests for $\mu$ when $\sigma$ is unknown.

---

# Snedecor's F-Distribution

Suppose $U$ and $V$ are two **independent** Chi-Square random variables with $n_1$ and $n_2$ degrees of freedom, respectively ($U \sim \chi^2(n_1)$, $V \sim \chi^2(n_2)$). The **F-statistic** is defined as the ratio of these variables, each divided by its degrees of freedom:

$$
F = \frac{U/n_1}{V/n_2}
$$

This statistic $F$ follows **Snedecor's F-distribution** with $(n_1, n_2)$ degrees of freedom, denoted $F \sim F(n_1, n_2)$.

*   $n_1$: Numerator degrees of freedom
*   $n_2$: Denominator degrees of freedom

## PDF of F-Distribution

The PDF of the F-distribution with $(n_1, n_2)$ degrees of freedom is:
$$
f(F; n_1, n_2) = \frac{\Gamma\left(\frac{n_1+n_2}{2}\right)}{\Gamma\left(\frac{n_1}{2}\right) \Gamma\left(\frac{n_2}{2}\right)} \left(\frac{n_1}{n_2}\right)^{n_1/2} \frac{F^{n_1/2 - 1}}{\left(1 + \frac{n_1}{n_2} F\right)^{(n_1+n_2)/2}}, \quad F > 0
$$
*(Note: The constant involves the Beta function: $\frac{(n_1/n_2)^{n_1/2}}{B(n_1/2, n_2/2)}$)*

## Application: Comparing Two Population Variances

Let $s_1^2$ and $s_2^2$ be the sample variances from two independent random samples of sizes $n_1$ and $n_2$, taken from two normal populations $N(\mu_1, \sigma_1^2)$ and $N(\mu_2, \sigma_2^2)$, respectively.
We know:
*   $U = \frac{(n_1-1)s_1^2}{\sigma_1^2} \sim \chi^2(n_1-1)$
*   $V = \frac{(n_2-1)s_2^2}{\sigma_2^2} \sim \chi^2(n_2-1)$

Applying the definition of the F-statistic:
$$
F = \frac{U/(n_1-1)}{V/(n_2-1)} = \frac{\frac{(n_1-1)s_1^2/\sigma_1^2}{n_1-1}}{\frac{(n_2-1)s_2^2/\sigma_2^2}{n_2-1}} = \frac{s_1^2 / \sigma_1^2}{s_2^2 / \sigma_2^2}
$$
This statistic follows an F-distribution with $(n_1-1, n_2-1)$ degrees of freedom:
$$
\frac{s_1^2 / \sigma_1^2}{s_2^2 / \sigma_2^2} \sim F(n_1-1, n_2-1)
$$
A common application is testing the hypothesis $H_0: \sigma_1^2 = \sigma_2^2$. If this hypothesis is true, the statistic simplifies to:
$$
F = \frac{s_1^2}{s_2^2} \sim F(n_1-1, n_2-1)
$$
This is used in hypothesis testing for equality of variances and is a basis for ANOVA (Analysis of Variance).

## Characteristics of F-Distribution

1.  **Range:** The F-distribution is defined for $F > 0$.
2.  **Shape:** It is unimodal and positively skewed (skewness decreases as $n_1$ and $n_2$ increase).
3.  **Mean:**
    $$
    E[F] = \frac{n_2}{n_2-2}, \quad \text{provided } n_2 > 2
    $$
    (The mean is undefined if $n_2 \le 2$).
4.  **Variance:**
    $$
    \text{Var}(F) = \frac{2n_2^2(n_1+n_2-2)}{n_1(n_2-2)^2(n_2-4)}, \quad \text{provided } n_2 > 4
    $$
    (The variance is undefined if $n_2 \le 4$).
5.  **Reciprocal Property:** If $F \sim F(n_1, n_2)$, then $1/F \sim F(n_2, n_1)$. This is useful for finding critical values for lower-tail tests.

---
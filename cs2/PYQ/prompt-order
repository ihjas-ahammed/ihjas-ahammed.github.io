TEXT

# Discrete Random Variables and Their Probability Distribution

## Random Variable and Probability Distribution

A random variable is a function from a sample space to a real number.

For example, consider tossing two coins at the same time. The sample space is:

$$ S = \{HH, HT, TH, TT\} $$

Letâ€™s define a random variable $ x $ as the number of heads. The values of $ x $ and their probabilities are:

|       | HH  | HT  | TH  | TT  |
|-------|-----|-----|-----|-----|
| $ x $ | 2   | 1   | 1   | 0   |
| $ p $ | $ P(x=2) = \frac{1}{4} $ | $ P(x=1) = \frac{1}{2} $ | $ P(x=1) = \frac{1}{2} $ | $ P(x=0) = \frac{1}{4} $ |

This table represents the **probability distribution** (similar to a frequency distribution).

The **expectation** of $ x $, denoted $ E(x) $, is the mean of the probability distribution:

$$ E(x) = \sum x_i P(x = x_i) $$

In this case:

$$ E(x) = 2 \cdot \frac{1}{4} + 1 \cdot \frac{1}{2} + 1 \cdot \frac{1}{2} + 0 \cdot \frac{1}{4} = 0.5 + 0.5 + 0.5 + 0 = 1 $$

Random variables are also called *chance variables* or *stochastic variables*. If $ x_1 $ and $ x_2 $ are random variables and $ c $ is a constant, then:

1. $ c x_1 $ is a random variable
2. $ x_1 + x_2 $ is a random variable
3. $ x_1 - x_2 $ is a random variable
4. $ \max(x_1, x_2) $ is a random variable
5. $ \min(x_1, x_2) $ is a random variable

---

## Probability Mass Functions

The probability distribution of a discrete random variable $ X $ is a list of distinct values $ x_i $ with their associated probabilities. The function $ f(x_i) $ or $ P(X = x_i) $ is called the **probability mass function (PMF)**, provided:

1. $ f(x_i) \geq 0 $
2. $ \sum f(x_i) = 1 $

A probability mass function can be represented as a histogram.

For the coin toss example, the PMF is:

- $ P(x = 0) = \frac{1}{4} $
- $ P(x = 1) = \frac{1}{2} $ (since $ P(HT) + P(TH) = \frac{1}{4} + \frac{1}{4} = \frac{1}{2} $ )
- $ P(x = 2) = \frac{1}{4} $

---

## Distribution Functions

For any random variable $ X $, the **cumulative distribution function (CDF)** is defined as:

$$ F(x) = P(X \leq x) $$

If $ X $ is a discrete random variable with PMF $ P(x) $, then:

$$ F(x) = P(X \leq x) = \sum_{x_i \leq x} P(x_i) $$

### Properties

If $ F(x) $ is the CDF of $ X $:

1. It is defined for all real values of $ x $.
2. $ 0 \leq F(x) \leq 1 $.
3. $ F(-\infty) = 0 $ and $ F(\infty) = 1 $.
4. If $ a < b $, then $ F(a) \leq F(b) $ (i.e., $ F $ is non-decreasing).
5. For discrete random variables, the graph of $ F(x) $ is a step function.

For the coin toss example:

- $ F(0) = P(x \leq 0) = P(x = 0) = \frac{1}{4} $
- $ F(1) = P(x \leq 1) = P(x = 0) + P(x = 1) = \frac{1}{4} + \frac{1}{2} = \frac{3}{4} $
- $ F(2) = P(x \leq 2) = P(x = 0) + P(x = 1) + P(x = 2) = \frac{1}{4} + \frac{1}{2} + \frac{1}{4} = 1 $

---

## Mathematical Expectation

Every distribution of a random variable has associated parameters. The distribution function $ F(x) $ or the mass function $ f(x) $ completely characterizes the behavior of $ X $.

### Expectation of a Function of a Random Variable

Let $ g(x) $ be a function of a discrete random variable $ X $. The expected value is:

$$ E(g(x)) = \sum g(x_i) P(x_i) $$

### Properties of Expectation

1. **If $ C $ is a constant, $ E(C) = C $:**

   $$ E(C) = \sum C P(x_i) = C \sum P(x_i) = C \cdot 1 = C $$

2. **If $ g(x) $ is a function of $ X $ and $ C $ is a constant, then $ E[C g(x)] = C E[g(x)] $:**

   $$ E(C g(x)) = \sum C g(x_i) P(x_i) = C \sum g(x_i) P(x_i) = C E(g(x)) $$

3. **If $ a $ and $ b $ are constants, then $ E[aX + b] = a E(X) + b $:**

   $$ E(aX + b) = \sum (a x_i + b) P(x_i) = a \sum x_i P(x_i) + b \sum P(x_i) = a E(X) + b $$

4. **If $ g(x) $ and $ h(x) $ are functions of $ X $, then $ E[g(x) + h(x)] = E[g(x)] + E[h(x)] $:**

   $$ E(g(x) + h(x)) = \sum (g(x_i) + h(x_i)) P(x_i) = \sum g(x_i) P(x_i) + \sum h(x_i) P(x_i) = E(g(x)) + E(h(x)) $$

---

## Moments

### Raw Moments

The $ r $-th raw moment about a value $ x_0 $ is:

$$ \mu_r' = E[(X - x_0)^r] = \sum (x_i - x_0)^r P(x_i) $$

If $ x_0 = 0 $, the $ r $-th raw moment about the origin is:

$$ \mu_r' = E(X^r) = \sum x_i^r P(x_i) $$

For $ r = 1 $:

$$ \mu_1' = E(X) = \sum x_i P(x_i) $$

This is the mean of the random variable $ X $.

### Central Moments

The $ r $-th central moment (about the expected value) is:

$$ \mu_r = E[(X - E(X))^r] $$

- For $ r = 1 $:

  $$ \mu_1 = E(X - E(X)) = E(X) - E(X) = 0 $$

- For $ r = 2 $:

  $$ \mu_2 = E[(X - E(X))^2] $$

  This is the **variance** of $ X $.

### Relation Between Raw and Central Moments

The $ r $-th central moment can be expressed in terms of raw moments:

$$ \mu_r = \mu_r' - \binom{r}{1} \mu_{r-1}' \mu_1' + \binom{r}{2} \mu_{r-2}' (\mu_1')^2 + \cdots + (-1)^r (\mu_1')^r $$

- For $ r = 1 $:

  $$ \mu_1 = \mu_1' - \binom{1}{1} \mu_0' \mu_1' = \mu_1' - 1 \cdot 1 \cdot \mu_1' = 0 $$

- For $ r = 2 $:

  $$ \mu_2 = \mu_2' - \binom{2}{1} \mu_1' \mu_1' + \binom{2}{2} \mu_0' (\mu_1')^2 = \mu_2' - 2 (\mu_1')^2 + 1 \cdot (\mu_1')^2 = \mu_2' - (\mu_1')^2 $$

  This is the variance.

- For $ r = 3 $:

  $$ \mu_3 = \mu_3' - \binom{3}{1} \mu_2' \mu_1' + \binom{3}{2} \mu_1' (\mu_1')^2 - \binom{3}{3} (\mu_1')^3 = \mu_3' - 3 \mu_2' \mu_1' + 3 (\mu_1')^3 - (\mu_1')^3 = \mu_3' - 3 \mu_2' \mu_1' + 2 (\mu_1')^3 $$

---

## Moment Generating Function (MGF)

The moment generating function of a random variable $ X $ is:

$$ M_X(t) = E(e^{tX}) = \sum e^{t x_i} P(x_i) $$

### Properties

1. If $ c $ is a constant: $ M_{cX}(t) = M_X(ct) $
2. If $ a $ and $ b $ are constants: $ M_{aX+b}(t) = e^{bt} M_X(at) $
3. For independent $ X $ and $ Y $: $ M_{X+Y}(t) = M_X(t) \cdot M_Y(t) $
4. $ M_X(0) = 1 $
5. The MGF uniquely determines the probability distribution (if it exists).
6. The $ r $-th raw moment is: $ \mu_r' = \frac{d^r}{dt^r} M_X(t) \big|_{t=0} $

   - $ \frac{d}{dt} M_X(t) \big|_{t=0} = \mu_1' $
   - $ \frac{d^2}{dt^2} M_X(t) \big|_{t=0} = \mu_2' $

### Variance Relations

- $ \mu_2 = \mu_2' - (\mu_1')^2 $
- $ \mu_2 = E[(X - E(X))^2] $
- $ V(aX) = a^2 V(X) $

---

## Properties of Variance

1. $ V(aX) = E[(aX - E(aX))^2] = a^2 V(X) $
2. For random variables $ X $ and $ Y $:

   $$ V(X + Y) = V(X) + V(Y) + 2 \text{Cov}(X, Y) $$

3. If $ X $ and $ Y $ are independent:

   $$ V(X + Y) = V(X) + V(Y) $$

---

## Covariance

The covariance of $ X $ and $ Y $ is:

$$ \text{Cov}(X, Y) = E[(X - E(X))(Y - E(Y))] = E(XY) - E(X)E(Y) $$

### Proof for Independent Variables

If $ X $ and $ Y $ are independent:

$$ E(XY) = E(X)E(Y) $$

Thus:

$$ \text{Cov}(X, Y) = E(XY) - E(X)E(Y) = E(X)E(Y) - E(X)E(Y) = 0 $$

---

# Binomial Distribution

A discrete random variable $ X $ follows a binomial distribution if its PMF is:

$$ P(X = x) = \binom{n}{x} p^x q^{n-x}, \quad x = 0, 1, 2, \ldots, n $$

where $ q = 1 - p $, and otherwise $ P(X = x) = 0 $. Here, $ n $ and $ p $ are the parameters of the binomial distribution. We denote this as:

$$ X \sim B(n, p) $$

## Mean

$$ E(X) = \sum_{x=0}^n x P(x) = \sum_{x=0}^n x \binom{n}{x} p^x q^{n-x} $$

Simplify:

$$ E(X) = \sum_{x=1}^n x \frac{n!}{x! (n-x)!} p^x q^{n-x} = \sum_{x=1}^n n \frac{(n-1)!}{(x-1)! (n-x)!} p^x q^{n-x} $$

Let $ k = x - 1 $:

$$ E(X) = n p \sum_{k=0}^{n-1} \binom{n-1}{k} p^k q^{n-1-k} = n p (p + q)^{n-1} = n p \cdot 1 = n p $$

**Mean = $ n p $**

## Variance

$$ V(X) = E(X^2) - [E(X)]^2 $$

First, compute $ E(X^2) $:

$$ E(X^2) = \sum_{x=0}^n x^2 P(x) = \sum_{x=0}^n [x(x-1) + x] P(x) = \sum_{x=0}^n x(x-1) P(x) + \sum_{x=0}^n x P(x) $$

$$ = \sum_{x=2}^n x(x-1) \binom{n}{x} p^x q^{n-x} + n p $$

$$ = n(n-1) p^2 \sum_{x=2}^n \binom{n-2}{x-2} p^{x-2} q^{n-x} + n p = n(n-1) p^2 (p + q)^{n-2} + n p = n(n-1) p^2 + n p $$

Then:

$$ V(X) = E(X^2) - [E(X)]^2 = [n(n-1) p^2 + n p] - (n p)^2 = n^2 p^2 - n p^2 + n p - n^2 p^2 = n p (1 - p) = n p q $$

**Variance = $ n p q $**

## Moment Generating Function

$$ M_X(t) = E(e^{tX}) = \sum_{x=0}^n e^{tx} \binom{n}{x} p^x q^{n-x} = \sum_{x=0}^n \binom{n}{x} (p e^t)^x q^{n-x} = (q + p e^t)^n $$

---

# Poisson Distribution

A random variable $ X $ follows a Poisson distribution if its PMF is:

$$ P(X = x) = \frac{e^{-\lambda} \lambda^x}{x!}, \quad x = 0, 1, 2, \ldots $$

otherwise, $ P(X = x) = 0 $. Here, $ \lambda $ is the parameter.

## Mean and Variance

$$ E(X) = \sum_{x=0}^\infty x \frac{e^{-\lambda} \lambda^x}{x!} = e^{-\lambda} \sum_{x=1}^\infty \frac{\lambda^x}{(x-1)!} = e^{-\lambda} \lambda \sum_{k=0}^\infty \frac{\lambda^k}{k!} = e^{-\lambda} \lambda e^\lambda = \lambda $$

**Mean = $ \lambda $**

$$ E(X^2) = \sum_{x=0}^\infty x^2 \frac{e^{-\lambda} \lambda^x}{x!} = \sum_{x=0}^\infty [x(x-1) + x] \frac{e^{-\lambda} \lambda^x}{x!} = e^{-\lambda} \lambda^2 \sum_{x=2}^\infty \frac{\lambda^{x-2}}{(x-2)!} + \lambda = \lambda^2 + \lambda $$

$$ V(X) = E(X^2) - [E(X)]^2 = (\lambda^2 + \lambda) - \lambda^2 = \lambda $$

**Variance = $ \lambda $**

## Moment Generating Function

$$ M_X(t) = E(e^{tX}) = \sum_{x=0}^\infty e^{tx} \frac{e^{-\lambda} \lambda^x}{x!} = e^{-\lambda} \sum_{x=0}^\infty \frac{(\lambda e^t)^x}{x!} = e^{-\lambda} e^{\lambda e^t} = e^{\lambda (e^t - 1)} $$

## Moments: Poisson Distribution

### Moment-Generating Function
The moment-generating function (MGF) for a random variable \( X \) is given as:
$$ M_X(t) = e^{-\lambda} e^{\lambda e^t} $$
This is the MGF of a Poisson random variable \( X \) with parameter \( \lambda \).

### First Derivative
The first derivative of the MGF is computed to find the mean:
$$ M'_X(t) = \frac{d}{dt} M_X(t) = e^{-\lambda} e^{\lambda e^t} \cdot \lambda e^t $$
Evaluating at \( t = 0 \):
$$ M'_X(0) = e^{-\lambda} e^{\lambda} \cdot \lambda = \lambda $$
Thus, the mean of the distribution is:
$$ E[X] = \lambda $$

### Second Derivative
The second derivative of the MGF is computed to find the second moment:
$$ M''_X(t) = \frac{d}{dt} \left( e^{-\lambda} e^{\lambda e^t} \cdot \lambda e^t \right) $$
Using the product rule:
- Let \( u = e^{\lambda e^t} \), \( v = \lambda e^t \)
- Derivatives:
  $$ \frac{du}{dt} = e^{\lambda e^t} \cdot \lambda e^t $$
  $$ \frac{dv}{dt} = \lambda e^t $$
- Applying the product rule:
  $$ \frac{d}{dt} (u \cdot v) = u' v + u v' = (e^{\lambda e^t} \cdot \lambda e^t) \cdot (\lambda e^t) + e^{\lambda e^t} \cdot (\lambda e^t) $$
  $$ = e^{\lambda e^t} \cdot \lambda e^t \left( \lambda e^t + 1 \right) $$
Thus:
$$ M''_X(t) = e^{-\lambda} \left[ (e^{\lambda e^t} \cdot \lambda e^t) \cdot \lambda e^t + e^{\lambda e^t} \cdot \lambda e^t \right] $$
Evaluating at \( t = 0 \):
$$ M''_X(0) = e^{-\lambda} \left[ e^{\lambda} \cdot \lambda^2 + e^{\lambda} \cdot \lambda \right] = e^{-\lambda} e^{\lambda} (\lambda^2 + \lambda) = \lambda^2 + \lambda $$
So, the second moment is:
$$ E[X^2] = \lambda^2 + \lambda $$

### Variance Calculation
The variance is calculated using the formula:
$$ V(X) = E[X^2] - (E[X])^2 $$
Substituting the values:
$$ V(X) = (\lambda^2 + \lambda) - \lambda^2 = \lambda $$


PYQ

Okay, here are the questions from the provided exam papers that are relevant to the provided text on discrete random variables, probability distributions, expectation, moments, binomial, and Poisson distributions.  I've rewritten them in Markdown, using MathJax for the mathematical notation.

**Relevant Questions (Formatted for Markdown/MathJax):**

From C 4401:

1.  What is the expected profit for a seller per item, if he is selling a large number of this item per day with various profits Rs. 2, 3 or 4 according to the buyer with a probability 0.2, 0.35 and 0.45 respectively ?
2.  For any random variable X, show that $V(X)$ is always non-negative.
3.  Define characteristic function of a random variable X.
4.   For two independent random variables X and Y, show that $V(X - Y) = V(X) + V(Y)$.
5.  Define a Bernoulli random variable.
6.  Identify the parameters of X following binomial distribution with mean 12 and variance 3.
7.  Obtain the m.g.f. of X following geometric distribution with parameter $p$.
8. If X is the number shown when an unbiased die is thrown. Name the probability distribution of X. Write the p.m.f. of X and find $E(X)$.
9. If the variance of X following Poisson distribution is 5, find $P(X = 5)$.
10. Define negative binomial distribution.
11. Explain two methods of finding raw moments of X, when moment generating function of X exists.
12. If $\phi_X(t)$ is the characteristic function of X, prove that $|\phi_X(t)| \leq 1$.
13. State and prove the additive property of binomial distribution.
14. If $P(X=2) = P(X=3)$, where X follows Poisson distribution, find the m.g.f. of X.

From C 4402:

1.  Define (a) Random experiment ; (b) Event.
2.  Define probability density function and state any *two* of its properties.
3.  Obtain the distribution function of X, with p.d.f. $f(x) = 3x^2$, for $0 < x < 1$.
4.  Find the value of $k$, if $f(x) = \binom{k}{2}^x$, for $x = 1, 2, 3...$ is the probability mass function of X.
5. If $E(X)=2, E(X^2)=8$, find $V(3X-2)$.
6. Obtain the mean and the variance of a random variable X with m.g.f $M_X(t)=(1-t)^{-1}, t<1$
7. Define characteristic function of a random variable and state its advantage over m.g.f.
8.  Mentioning the underlying assumptions clearly, state axiomatic definition of probability.

    Using this definition establish $0 \leq P(A) \leq 1$ for an event A.
9.  The p.m.f. of X, $f(x) = \frac{2x^2 - 1}{k}$, for $x = 1, 2, 3, 4$ and $f(x) = 0$ *elsewhere* (i) Find $k$ ; (ii) Write the distribution function $F(x)$.
10. Given the p.d.f. of X as $f(x) = 1$, for $0 < x < 1$. Find the p.d.f. of $Y = -2 \log_e X$.
11. In a game three balls are drawn from a box containing 5 white and 7 black balls. 10 points are given for each white ball drawn and 5 points are given for each black ball drawn. Calculated the expected points per game for a long run of the game.
12. For X with p.d.f. $f(x) = kx(2 - x)$, for $0 < x < 1 ; f(x) = 0$, elsewhere. Obtain (a) $k$ ; (b) Mean and variance of X.
13. For two random variables X and Y, prove that (i) $V(X - Y) = V(X) + V(Y) - 2Cov(X, Y)$ ; (ii) $Cov(X - a, Y - b) = Cov(X, Y)$, where $a$ and $b$ are two constants.
14. (a) Cauchy-Schwartz Inequality for two random variables X and Y.
    (b) Using this inequality prove $-1 \leq r_{XY} \leq +1$, where $r_{XY}$ is the coefficient of correlation between X and Y.

From C 4185:
1. Let x denote the number of heads obtained when a fair coin is tossed thrice ; then $P(x=1)$ is:
2. Two dice are rolled. Let x be the maximum of the numbers that turn-up then $P(x=5)=$
3.  If x is the number of heads obtained in tossing 3 coins, $E(x)=$
4. For a binomial distribution $P(x)={n \choose x}p^xq^{n-x}, x=0,1,2,...,n$, which of the following is incorrect:
5. The measure of skewness $\beta_1$ is related to the central moments as :
6. The curve is said to be leptokurtic if:
7. In binomial distribution, the variance $\sigma^2$ and mean $\mu$ are related by :
8. The range of the Bernoulli random variable is the set :
9. The variance of the uniform (discrete) probability distribution is given by :
10. Following is the probability distribution of a discrete random variable $x$. What is $E(2x + 3)$?

    | $x$      | -3   | -2   | -1   | 0  | 1    | 2    | 3    |
    | :------- | :--- | :--- | :--- | :- | :--- | :--- | :--- |
    | $P(x)$   | 0.05 | 0.10 | 0.30 | 0  | 0.30 | 0.15 | 0.10 |
11. For the probability distribution given in problem (10), what is $V(2x + 3)$?
12. Let $x$ be the number of heads obtained in four tosses of a fair coin. Find $E(x)$ :
13. Variance of an exponential distribution is:
14. Name the coefficient of $\frac{(it)^r}{r!}$ in the expansion of characteristic function.
15. Name the discrete distribution having memoryless property.
16. If X and Y are two independent variables, the conditional distribution of X given $Y = y, f(x|y) =$ __________.
17. If $X \sim B(n, p)$, the distribution of $y = n - X$ is __________.
18.  The variance of the rectangular distribution $f(x) = \frac{1}{b - a} ; a \leq x \leq b$ is equal to __________.
19. If X, Y and Z are three random variables, then $cov(X + Y, Z) = cov(X, Z) + cov(Y, Z)$.
20. For a geometric distribution mean is always less than the variance.
21. Define mathematical expectation of a random variable.
22. What are the properties of moment generating function ?
23. Define conditional variance.
24. Define geometric distribution.
25. State and prove the addition theorem of expectation.
26. What are the physical conditions for which binomial distribution is used ?
27. Show that in a Poisson distribution with unit mean, mean deviation about mean is $\frac{2}{e}$ times the standard deviation.
28. What is the expectation of the number of failures before the first success in an infinite series of independent trials with constant probability $p$ of success in each trial ?
29. Find the m.g.f. of the random variables whose moments are (i) $\mu_r'$, $(r+1)!2^r$ and (ii) $\mu_r, r!$
30.  A car hire firm has two cars which it hires out day by day. The number of demands for a car on each day is distributed as Poisson variate with mean 1.5. Calculate the proportion of days on which (i) neither car is used and (ii) some demand is refused
31. Derive Poisson distribution as a limiting case of binomial distribution.

From C 4184:

1.  For the bivariate distribution function $(X,Y), F_{(X,Y)}(\infty,\infty)=$__________.
2.  If X and Y two independent random variables, $E(X/Y)=$__________.
3.  For the bivariate distribution function $(X,Y), M_{X,Y}(t_1,t_2)=$__________.
4. The first and second moments of $X$ about 5 is 2 and 6, then $V(X)=$
5. For two random variables $X$ and $Y, V(\alpha X -bY)=$__________.
6. $Cov(\bar{X}+a,Y+b)=$__________.
7.  Expectation of a Bernoulli random variable with parameter $p$ is__________.
8.  For a geometric random variable $X, P(X=x+1)=$__________$ \times P(X=x)$.
9. Show that the 1st central moment about of a random variable $X$ is zero.
10. Define independence of two random variables $X$ and $Y$
11. Show that the characteristic function $\phi_x(t)/t=0$ is equal to $1$.
12. Define the coefficient of kurtosis based on moments.
13. Find the values of the parameters of a binomial random variable with mean 4 and variance 3.
14. Find $P(X=0)$ for a Poisson random variable with mean 5.
15. State and prove multiplication theorem on expectation.
16. Find the m.g.f. of $X$ with p.d.f $f(x)=\frac{1}{2}e^{-|x|}, -\infty < x < \infty$.
17.  $X$ and $Y$ are independent random variables following Poisson distribution with parameters $\lambda$ and $\mu$ respectively, where $V(X)=2$ and $V(Y)=3$. Find $P(X+Y>0)$.
18. For a random variable $X$, prove that $E(X)=E[E(X|Y)]$.
19. Obtain the mode of $X$ following binomial distribution with parameters $n$ and $p$.
20. If $X \sim P(\lambda)$; prove that $\mu_{r+1}=r\lambda\mu_{r-1}+\lambda\frac{d}{d\lambda}\mu_r$, where $\mu_{r-1}, \mu_r, \mu_{r+1}$ respectively be the $(r-1)^{th}, r^{th}$ and $(r+1)^{th}$ central moments of $X$.
21. If X and Y independent random variables such that $P(X=r)=P(Y=r)=q^rp, r=0,1,2,...$
where, $p$ and $q$ are positive numbers such that $p+q=1$. Find (i) the distribution of $X+Y$; (ii) The conditionai distribution of $X$ given $X+Y$.

These questions cover the key concepts from the provided text and are now properly formatted for use in a Markdown document with mathematical typesetting.


rewrite the avbove PYQ in topic wise based on the TEXT also it's 3 mark for section A,  5 for B, 11 for C

but for qa with section E it's 1 for A, 2 for B, 3 for C, 4 for D and 10 for E

so just write like marks in  bracket after each question